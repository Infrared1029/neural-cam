{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'train2017' # coco 2017 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "images = pd.DataFrame({\n",
    "    \"images_paths\": [os.path.join(TRAIN_PATH, img_path) for img_path in os.listdir('train2017')]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding,\n",
    "                 use_activation=True,\n",
    "                use_instance_norm=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True) if use_instance_norm else nn.Identity(),\n",
    "            nn.ReLU() if use_activation else nn.Identity()\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "        \n",
    "        \n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "        ConvBlock(in_channels, in_channels, kernel_size, stride, padding),\n",
    "        ConvBlock(in_channels, in_channels, kernel_size, stride, padding, use_activation=False)    \n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.res_block(x)\n",
    "        \n",
    "\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            ConvBlock(3, 32, (9, 9), 1, 4), #32x256x256\n",
    "            ConvBlock(32, 64, (3, 3), 2, 1), #64x128x128\n",
    "            ConvBlock(64, 128, (3, 3), 2, 1), #128x64x64\n",
    "            \n",
    "            *[ResBlock(128, (3, 3), 1, 1) for i in range(5)], #128x64x64\n",
    "            \n",
    "            nn.Upsample(scale_factor=2), #128x128x128\n",
    "            ConvBlock(128, 64, (3, 3), 1, 1), #64x128x128\n",
    " \n",
    "            nn.Upsample(scale_factor=2), #64x256x256\n",
    "            ConvBlock(64, 32, (3, 3), 1, 1), #32x256x256\n",
    "            \n",
    "            ConvBlock(32, 3, (9, 9), 1,  4, use_activation=False, use_instance_norm=False), #3x256x256\n",
    "            \n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram\n",
    "\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    # normalize using imagenet mean and std\n",
    "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    batch = batch.div_(255.0)\n",
    "    return (batch - mean) / std\n",
    "\n",
    "\n",
    "def tv_loss(img):\n",
    "    return 0.5 * (torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]).mean() +\n",
    "                    (torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]).mean()))\n",
    "\n",
    "\n",
    "\n",
    "class Vgg16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg16 = torchvision.models.vgg16(pretrained=True).features\n",
    "        \n",
    "        self.style_layers = ['3', '8', '15', '22']\n",
    "        self.content_layers = ['15']\n",
    "        \n",
    "        \n",
    "        for param in self.vgg16.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        layer_features = {}\n",
    "        for name, layer in self.vgg16.named_children():\n",
    "            x = layer(x)\n",
    "            if name in self.style_layers + self.content_layers:\n",
    "                layer_features[name] = x\n",
    "                \n",
    "            if name == '22':\n",
    "                break\n",
    "                \n",
    "        return layer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, gray=False):\n",
    "        self.data = data\n",
    "        self.gray = gray\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_arr = load_image(self.data.iloc[index][0], gray=self.gray)\n",
    "        image_arr = torch.from_numpy(image_arr)\n",
    "        image_arr = image_arr.permute((2,0,1))\n",
    "#         image_arr = preprcoess_image(image_arr)\n",
    "#         print(image_arr.dtype)\n",
    "        return image_arr.float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    \n",
    "def load_image(image_path: str, gray=False):\n",
    "    '''loads an image using opencv and converts it to RGB isntead of opencv's BGR then returns it.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): path to the image\n",
    "        \n",
    "    Returns:\n",
    "        image_arr (numpy array)\n",
    "    '''\n",
    "    image_arr = cv2.imread(image_path)\n",
    "    if gray:\n",
    "        image_arr = cv2.cvtColor(image_arr, cv2.COLOR_BGR2GRAY)\n",
    "        image_arr = np.stack([image_arr, image_arr, image_arr], axis=2)\n",
    "    else: \n",
    "        image_arr = cv2.cvtColor(image_arr, cv2.COLOR_BGR2RGB)\n",
    "    image_arr = cv2.resize(image_arr, (256, 256))\n",
    "    \n",
    "    return image_arr\n",
    "\n",
    "\n",
    "def denormalize_img(img_arr):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    x = img_arr.cpu().detach()\n",
    "    x = (x*std) + mean\n",
    "    return x.permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight = 1e4\n",
    "style_weight = 5e9\n",
    "tv_weight = 8.5e-5\n",
    "\n",
    "STYLE_IMAGE = 'scream.jpg'\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_dataset = ImageDataset(images)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4)\n",
    "\n",
    "transformer = TransformerNet().to(device)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), 1e-3)\n",
    "\n",
    "vgg = Vgg16().to(device)\n",
    "\n",
    "style = torch.from_numpy(load_image(f'style/{STYLE_IMAGE}.jpg')).permute(2, 0, 1).unsqueeze(0).float()\n",
    "style = style.repeat(4, 1, 1, 1).to(device)\n",
    "\n",
    "features_style = vgg(normalize_batch(style))\n",
    "gram_style = [gram_matrix(y) for y in features_style.values()]\n",
    "\n",
    "\n",
    "for e in range(1):\n",
    "    \n",
    "    transformer.train()\n",
    "\n",
    "    count = 0\n",
    "    for batch_id, x in enumerate(train_loader):\n",
    "        n_batch = len(x)\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        y = transformer(x)\n",
    "        y_temp = y.detach().clone()\n",
    "\n",
    "        y = normalize_batch(y)\n",
    "        x = normalize_batch(x)\n",
    "\n",
    "        features_y = vgg(y)\n",
    "        features_x = vgg(x)\n",
    "  \n",
    "        content_loss = content_weight * F.mse_loss(features_y['8'], features_x['8'])\n",
    "    \n",
    "\n",
    "        style_loss = 0.\n",
    "\n",
    "        for ft_y, gm_s in zip(features_y.values(), gram_style):\n",
    "            gm_y = gram_matrix(ft_y)\n",
    "            style_loss += F.mse_loss(gm_y, gm_s)\n",
    "        style_loss *= style_weight\n",
    "        \n",
    "    \n",
    "        \n",
    "        total_variation_loss = tv_loss(y)\n",
    "        total_variation_loss *= tv_weight\n",
    "\n",
    "        total_loss = content_loss + style_loss + total_variation_loss\n",
    "                \n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'{batch_id}, {total_loss.item()}')\n",
    "        if batch_id%10 ==0 :\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 7))\n",
    "            axs[0].imshow(denormalize_img(y[0]))\n",
    "            axs[1].imshow(denormalize_img(x[0]))\n",
    "            axs[0].axis('off')\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            \n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), f'models/{STYLE_IMAGE}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
